{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Super Resolution\n",
    "### Classical Upsampling vs. SRCNN (Deep Learning) — Frame-by-Frame Pipeline\n",
    "\n",
    "This notebook builds a complete video super resolution pipeline:\n",
    "\n",
    "1. **Data pipeline** — downsample frames as LR input, HR as ground truth\n",
    "2. **Classical baselines** — nearest neighbour, bicubic, Lanczos\n",
    "3. **SRCNN** — Super Resolution CNN in PyTorch (Dong et al., 2014)\n",
    "4. **Quantitative evaluation** — PSNR & SSIM across all methods\n",
    "5. **Video processing** — frame-by-frame inference, video reconstruction\n",
    "\n",
    "---\n",
    "**Dependencies:**\n",
    "```\n",
    "pip install torch torchvision numpy scipy matplotlib scikit-image opencv-python Pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Download assets from GitHub (Colab setup) ─────────────────────────────────\n",
    "import os\n",
    "if not os.path.exists('clip.mp4'):\n",
    "    !wget -q https://raw.githubusercontent.com/preeti-chauhan/video-super-resolution/main/clip.mp4\n",
    "    print('Downloaded clip.mp4')\n",
    "print('Assets ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom skimage import data as skdata, color, transform\nfrom skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\nfrom skimage.transform import resize, rescale\nimport cv2\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {DEVICE}')\nprint(f'PyTorch: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## 1 · Data Pipeline\n\nWe use a real video clip (`clip.mp4`, 4K forest nature footage) as HR reference frames.\n\nPipeline:\n1. Extract Y (luminance) channel from frames for training (standard SRCNN approach)\n2. Downsample x SCALE_FACTOR to LR; bicubic-upsample back to HR size as SRCNN input\n3. Extract overlapping 33x33 patches into SRDataset\n4. 80/10/10 train/val/test split\n5. For video output: SRCNN on Y channel + bicubic Cb/Cr → full color output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Parameters ────────────────────────────────────────────────────────────────\nSCALE_FACTOR  = 3       # ← tune: 2, 3, or 4 (upscaling factor)\nPATCH_SIZE    = 33      # ← tune: SRCNN input patch size (must be odd)\nSTRIDE        = 14      # ← tune: patch extraction stride\nBATCH_SIZE    = 64\nNUM_EPOCHS    = 30      # ← tune: increase for better results\nLR            = 1e-4\n\nVIDEO_PATH = 'clip.mp4'\n\ndef load_frames_from_video(path):\n    \"\"\"Extract Y (luminance) channel for training + BGR color frames for visualization.\"\"\"\n    cap = cv2.VideoCapture(path)\n    y_frames    = []   # grayscale Y channel — used for training\n    bgr_frames  = []   # full color — used for video output\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        ycbcr = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb).astype(np.float32) / 255.0\n        y_frames.append(ycbcr[:, :, 0])\n        bgr_frames.append(frame)\n    cap.release()\n    return y_frames, bgr_frames\n\nhr_frames, bgr_frames = load_frames_from_video(VIDEO_PATH)\nprint(f'Loaded {len(hr_frames)} frames — Y shape: {hr_frames[0].shape}')\n\n# Preview 12 evenly-spaced color frames\nidxs = [int(i * len(bgr_frames) / 12) for i in range(12)]\nfig, axes = plt.subplots(2, 6, figsize=(18, 6))\nfor ax, idx in zip(axes.flat, idxs):\n    ax.imshow(cv2.cvtColor(bgr_frames[idx], cv2.COLOR_BGR2RGB))\n    ax.axis('off')\nfig.suptitle('HR Frames from clip.mp4 (color)', fontweight='bold')\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n    \"\"\"\n    Extracts overlapping patches from (LR_bicubic, HR) pairs.\n    SRCNN is trained on patches to keep memory manageable.\n    \"\"\"\n    def __init__(self, lr_frames, hr_frames, patch_size=33, stride=14):\n        self.lr_patches = []\n        self.hr_patches = []\n\n        for lr, hr in zip(lr_frames, hr_frames):\n            H, W = hr.shape\n            for r in range(0, H - patch_size + 1, stride):\n                for c in range(0, W - patch_size + 1, stride):\n                    self.lr_patches.append(lr[r:r+patch_size, c:c+patch_size])\n                    self.hr_patches.append(hr[r:r+patch_size, c:c+patch_size])\n\n    def __len__(self):\n        return len(self.lr_patches)\n\n    def __getitem__(self, idx):\n        lr = torch.from_numpy(self.lr_patches[idx]).unsqueeze(0)\n        hr = torch.from_numpy(self.hr_patches[idx]).unsqueeze(0)\n        return lr, hr\n\n# ── Generate LR/HR pairs ──────────────────────────────────────────────────────\ndef make_lr_hr_pairs(frames, scale):\n    \"\"\"Downsample HR frames to LR, then bicubic-upsample back to HR size.\"\"\"\n    lr_frames = []\n    for hr in frames:\n        H, W = hr.shape\n        lr_small = rescale(hr, 1.0 / scale, anti_aliasing=True, order=3).astype(np.float32)\n        lr_up    = resize(lr_small, (H, W), order=3, anti_aliasing=True).astype(np.float32)\n        lr_frames.append(lr_up)\n    return lr_frames\n\n# ── Train / val / test split (144 frames: 80% / 10% / 10%) ───────────────────\nn       = len(hr_frames)\nn_train = int(n * 0.8)\nn_val   = int(n * 0.1)\n\ntrain_hr = hr_frames[:n_train]\nval_hr   = hr_frames[n_train:n_train + n_val]\ntest_hr  = hr_frames[n_train + n_val:]\n\ntrain_lr = make_lr_hr_pairs(train_hr, SCALE_FACTOR)\nval_lr   = make_lr_hr_pairs(val_hr,   SCALE_FACTOR)\ntest_lr  = make_lr_hr_pairs(test_hr,  SCALE_FACTOR)\n\ntrain_ds = SRDataset(train_lr, train_hr, PATCH_SIZE, STRIDE)\nval_ds   = SRDataset(val_lr,   val_hr,   PATCH_SIZE, STRIDE)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\nprint(f'Train frames: {len(train_hr)} | Val frames: {len(val_hr)} | Test frames: {len(test_hr)}')\nprint(f'Train patches: {len(train_ds)} | Val patches: {len(val_ds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 · Classical Upsampling Baselines\n",
    "\n",
    "| Method | Order | Characteristic |\n",
    "|---|---|---|\n",
    "| Nearest neighbour | 0 | Fastest; blocky artefacts |\n",
    "| Bilinear | 1 | Smooth but blurry |\n",
    "| Bicubic | 3 | Standard baseline for SR |\n",
    "| Lanczos | 5 | Sharpest classical method |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classical_upsample(hr, scale):\n",
    "    \"\"\"Downsample HR, then upsample with each classical method.\"\"\"\n",
    "    lr_small = rescale(hr, 1.0 / scale, anti_aliasing=True, order=3)\n",
    "    target   = hr.shape\n",
    "    return {\n",
    "        'Nearest':  resize(lr_small, target, order=0).astype(np.float32),\n",
    "        'Bilinear': resize(lr_small, target, order=1).astype(np.float32),\n",
    "        'Bicubic':  resize(lr_small, target, order=3).astype(np.float32),\n",
    "        'Lanczos':  resize(lr_small, target, order=5).astype(np.float32),\n",
    "    }\n",
    "\n",
    "def compute_metrics(ref, candidate):\n",
    "    p = psnr(ref, candidate, data_range=1.0)\n",
    "    s = ssim(ref, candidate, data_range=1.0)\n",
    "    return p, s\n",
    "\n",
    "# Evaluate on first test frame\n",
    "hr_test  = test_hr[0]\n",
    "upscaled = classical_upsample(hr_test, SCALE_FACTOR)\n",
    "\n",
    "print(f'=== Classical Baselines (scale ×{SCALE_FACTOR}) ===')\n",
    "print(f'{\"Method\":<14} {\"PSNR (dB)\":>10} {\"SSIM\":>8}')\n",
    "print('-' * 36)\n",
    "for name, img in upscaled.items():\n",
    "    p, s = compute_metrics(hr_test, img)\n",
    "    print(f'{name:<14} {p:>10.2f} {s:>8.4f}')\n",
    "\n",
    "# Visualize\n",
    "imgs   = [hr_test, *upscaled.values()]\n",
    "titles = ['HR (ground truth)', *upscaled.keys()]\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for ax, img, title in zip(axes, imgs, titles):\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "fig.suptitle(f'Classical Upsampling Baselines (×{SCALE_FACTOR})', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 · SRCNN — Super Resolution CNN\n",
    "\n",
    "**SRCNN** (Dong et al., 2014) is the pioneering CNN for image super resolution.\n",
    "It maps a bicubic-upsampled LR image directly to HR using three conv layers:\n",
    "\n",
    "| Layer | Kernel | Filters | Role |\n",
    "|---|---|---|---|\n",
    "| Conv1 | 9×9 | 64 | Patch extraction & representation |\n",
    "| Conv2 | 1×1 | 32 | Non-linear mapping |\n",
    "| Conv3 | 5×5 | 1  | Reconstruction |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    \"\"\"SRCNN: Learning a Deep Convolutional Network for Image Super-Resolution.\n",
    "    Dong et al., ECCV 2014 / IEEE TPAMI 2015.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=9, padding=4)   # patch extraction\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=0)  # non-linear mapping\n",
    "        self.conv3 = nn.Conv2d(32, 1,  kernel_size=5, padding=2)  # reconstruction\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for layer in [self.conv1, self.conv2]:\n",
    "            nn.init.normal_(layer.weight, mean=0, std=0.001)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        nn.init.normal_(self.conv3.weight, mean=0, std=0.001)\n",
    "        nn.init.zeros_(self.conv3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "model = SRCNN().to(DEVICE)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'SRCNN parameters: {total_params:,}')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 · Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "# SRCNN paper uses different LRs per layer\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.conv1.parameters(), 'lr': LR},\n",
    "    {'params': model.conv2.parameters(), 'lr': LR},\n",
    "    {'params': model.conv3.parameters(), 'lr': LR * 0.1},  # smaller LR for last layer\n",
    "])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ── Train ──────────────────────────────────────────────────────────────────\n",
    "    model.train()\n",
    "    t_loss = 0.0\n",
    "    for lr_patch, hr_patch in train_loader:\n",
    "        lr_patch, hr_patch = lr_patch.to(DEVICE), hr_patch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        sr_patch = model(lr_patch)\n",
    "        loss = criterion(sr_patch, hr_patch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_loss += loss.item()\n",
    "\n",
    "    # ── Validate ───────────────────────────────────────────────────────────────\n",
    "    model.eval()\n",
    "    v_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for lr_patch, hr_patch in val_loader:\n",
    "            lr_patch, hr_patch = lr_patch.to(DEVICE), hr_patch.to(DEVICE)\n",
    "            sr_patch = model(lr_patch)\n",
    "            v_loss += criterion(sr_patch, hr_patch).item()\n",
    "\n",
    "    t_loss /= len(train_loader)\n",
    "    v_loss /= len(val_loader)\n",
    "    train_losses.append(t_loss)\n",
    "    val_losses.append(v_loss)\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save best model\n",
    "    if v_loss < best_val_loss:\n",
    "        best_val_loss = v_loss\n",
    "        torch.save(model.state_dict(), 'srcnn_best.pth')\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1:>3}/{NUM_EPOCHS}]  Train: {t_loss:.6f}  Val: {v_loss:.6f}  LR: {scheduler.get_last_lr()[0]:.2e}')\n",
    "\n",
    "# ── Load best weights & plot ───────────────────────────────────────────────────\n",
    "model.load_state_dict(torch.load('srcnn_best.pth', map_location=DEVICE))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses,   label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('SRCNN Training Curves')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f'Best val loss: {best_val_loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 · Evaluation — Classical vs. SRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def srcnn_upscale(model, lr_bicubic):\n",
    "    \"\"\"Run SRCNN inference on a full bicubic-upsampled frame.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        t = torch.from_numpy(lr_bicubic).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "        out = model(t).squeeze().cpu().numpy()\n",
    "    return np.clip(out, 0, 1).astype(np.float32)\n",
    "\n",
    "print(f'=== Super Resolution Benchmark (×{SCALE_FACTOR}) ===')\n",
    "print(f'{\"Method\":<14} {\"PSNR (dB)\":>10} {\"SSIM\":>8}')\n",
    "print('-' * 36)\n",
    "\n",
    "all_results = {}\n",
    "for hr, lr_bic in zip(test_hr, test_lr):\n",
    "    methods = {**classical_upsample(hr, SCALE_FACTOR), 'SRCNN': srcnn_upscale(model, lr_bic)}\n",
    "    for name, img in methods.items():\n",
    "        p, s = compute_metrics(hr, img)\n",
    "        all_results.setdefault(name, []).append((p, s))\n",
    "\n",
    "for name, scores in all_results.items():\n",
    "    avg_p = np.mean([s[0] for s in scores])\n",
    "    avg_s = np.mean([s[1] for s in scores])\n",
    "    marker = ' ◀ best' if name == 'SRCNN' else ''\n",
    "    print(f'{name:<14} {avg_p:>10.2f} {avg_s:>8.4f}{marker}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visual comparison ─────────────────────────────────────────────────────────\n",
    "hr_test  = test_hr[0]\n",
    "lr_bic   = test_lr[0]\n",
    "upscaled = classical_upsample(hr_test, SCALE_FACTOR)\n",
    "sr_out   = srcnn_upscale(model, lr_bic)\n",
    "\n",
    "imgs   = [hr_test, upscaled['Nearest'], upscaled['Bicubic'], upscaled['Lanczos'], sr_out]\n",
    "titles = ['HR (ground truth)', 'Nearest', 'Bicubic', 'Lanczos', 'SRCNN']\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for ax, img, title in zip(axes, imgs, titles):\n",
    "    p, s = compute_metrics(hr_test, img)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'{title}\\nPSNR={p:.1f} dB', fontsize=9)\n",
    "    ax.axis('off')\n",
    "fig.suptitle(f'Classical vs. SRCNN Super Resolution (×{SCALE_FACTOR})', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ── Zoomed crop comparison ────────────────────────────────────────────────────\n",
    "r, c, s = 80, 80, 80   # ← tune: crop region\n",
    "crops  = [img[r:r+s, c:c+s] for img in imgs]\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for ax, crop, title in zip(axes, crops, titles):\n",
    "    ax.imshow(crop, cmap='gray', interpolation='nearest')\n",
    "    ax.set_title(f'{title} (crop)')\n",
    "    ax.axis('off')\n",
    "fig.suptitle('Zoomed Crop — Fine Detail Comparison', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 · Video Super Resolution Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Generate LR video from clip.mp4 (color) ──────────────────────────────────\nSYNTHETIC_VIDEO = 'synthetic_lr.mp4'\nSR_VIDEO        = 'sr_output.mp4'\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nfps = cap.get(cv2.CAP_PROP_FPS)\nW   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nH   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nW_lr, H_lr = W // SCALE_FACTOR, H // SCALE_FACTOR\ncap.release()\n\ntmp_lr = '/tmp/lr_raw.mp4'\nout_lr = cv2.VideoWriter(tmp_lr, cv2.VideoWriter_fourcc(*'mp4v'), fps, (W_lr, H_lr), isColor=True)\n\nlr_preview = []\ncap = cv2.VideoCapture(VIDEO_PATH)\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n    lr_small = cv2.resize(frame, (W_lr, H_lr), interpolation=cv2.INTER_CUBIC)\n    out_lr.write(lr_small)\n    if len(lr_preview) < 5:\n        lr_preview.append(cv2.cvtColor(lr_small, cv2.COLOR_BGR2RGB))\ncap.release()\nout_lr.release()\n\nos.system(f'ffmpeg -y -i {tmp_lr} -vcodec libx264 -crf 23 -pix_fmt yuv420p {SYNTHETIC_VIDEO} -loglevel quiet')\nprint(f'LR video saved: {SYNTHETIC_VIDEO}  ({W_lr}x{H_lr} @ {fps}fps)')\n\n# Preview\nfig, axes = plt.subplots(1, 5, figsize=(18, 4))\nfor ax, frame in zip(axes, lr_preview):\n    ax.imshow(frame); ax.axis('off')\nfig.suptitle(f'Sample LR Frames (x{SCALE_FACTOR} downscaled, color)', fontweight='bold')\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def srcnn_upscale_color(model, bgr_frame, scale):\n    \"\"\"\n    Apply SRCNN on Y channel, bicubic upsample Cb/Cr, merge for full color output.\n    Input:  BGR frame (uint8)\n    Output: BGR frame (uint8) at scale x resolution\n    \"\"\"\n    H, W = bgr_frame.shape[:2]\n    W_hr, H_hr = W * scale, H * scale\n\n    # Convert to YCbCr float\n    ycbcr = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2YCrCb).astype(np.float32) / 255.0\n    y, cr, cb = ycbcr[:,:,0], ycbcr[:,:,1], ycbcr[:,:,2]\n\n    # Bicubic upsample Y for SRCNN input\n    y_lr_up = cv2.resize(cv2.resize(y, (W//scale, H//scale), interpolation=cv2.INTER_CUBIC),\n                         (W_hr, H_hr), interpolation=cv2.INTER_CUBIC)\n\n    # SRCNN on Y channel\n    y_sr = srcnn_upscale(model, y_lr_up)\n\n    # Bicubic upsample Cb and Cr\n    cr_up = cv2.resize(cr, (W_hr, H_hr), interpolation=cv2.INTER_CUBIC)\n    cb_up = cv2.resize(cb, (W_hr, H_hr), interpolation=cv2.INTER_CUBIC)\n\n    # Merge and convert back to BGR\n    ycbcr_hr = np.stack([y_sr, cr_up, cb_up], axis=2)\n    bgr_hr   = cv2.cvtColor((np.clip(ycbcr_hr, 0, 1) * 255).astype(np.uint8), cv2.COLOR_YCrCb2BGR)\n    return bgr_hr\n\ndef super_resolve_video(input_path, output_path, model, scale):\n    \"\"\"Read LR color video, apply SRCNN super resolution, write color HR MP4 output.\"\"\"\n    cap   = cv2.VideoCapture(input_path)\n    fps   = cap.get(cv2.CAP_PROP_FPS)\n    W_lr  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    H_lr  = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    W_hr, H_hr = W_lr * scale, H_lr * scale\n\n    tmp_out = '/tmp/sr_raw.mp4'\n    out = cv2.VideoWriter(tmp_out, cv2.VideoWriter_fourcc(*'mp4v'), fps, (W_hr, H_hr), isColor=True)\n\n    lr_preview = []\n    sr_preview = []\n    model.eval()\n\n    for i in range(total):\n        ret, frame = cap.read()\n        if not ret:\n            break\n        sr_frame = srcnn_upscale_color(model, frame, scale)\n        out.write(sr_frame)\n        if i < 5:\n            lr_preview.append(cv2.cvtColor(cv2.resize(frame, (W_hr, H_hr), interpolation=cv2.INTER_CUBIC), cv2.COLOR_BGR2RGB))\n            sr_preview.append(cv2.cvtColor(sr_frame, cv2.COLOR_BGR2RGB))\n\n    cap.release()\n    out.release()\n    os.system(f'ffmpeg -y -i {tmp_out} -vcodec libx264 -crf 23 -pix_fmt yuv420p {output_path} -loglevel quiet')\n    print(f'SR video saved: {output_path}  ({total} frames @ {W_hr}x{H_hr}, color)')\n    return lr_preview, sr_preview\n\nlr_preview, sr_preview = super_resolve_video(SYNTHETIC_VIDEO, SR_VIDEO, model, SCALE_FACTOR)\n\n# ── Visualize sample frames (color) ──────────────────────────────────────────\nfig, axes = plt.subplots(2, 5, figsize=(18, 6))\nfor i in range(5):\n    axes[0, i].imshow(lr_preview[i])\n    axes[0, i].set_title(f'LR bicubic frame {i+1}')\n    axes[0, i].axis('off')\n    axes[1, i].imshow(sr_preview[i])\n    axes[1, i].set_title(f'SRCNN SR frame {i+1}')\n    axes[1, i].axis('off')\nfig.suptitle('Video Super Resolution — SRCNN (Color)', fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7 · Save & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'srcnn_weights.pth')\n",
    "print('Model saved to srcnn_weights.pth')\n",
    "\n",
    "# ── Load (example) ────────────────────────────────────────────────────────────\n",
    "# model_loaded = SRCNN().to(DEVICE)\n",
    "# model_loaded.load_state_dict(torch.load('srcnn_weights.pth', map_location=DEVICE))\n",
    "# model_loaded.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}