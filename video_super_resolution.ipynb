{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Super Resolution\n",
    "### Classical Upsampling vs. SRCNN (Deep Learning) — Frame-by-Frame Pipeline\n",
    "\n",
    "This notebook builds a complete video super resolution pipeline:\n",
    "\n",
    "1. **Data pipeline** — downsample frames as LR input, HR as ground truth\n",
    "2. **Classical baselines** — nearest neighbour, bicubic, Lanczos\n",
    "3. **SRCNN** — Super Resolution CNN in PyTorch (Dong et al., 2014)\n",
    "4. **Quantitative evaluation** — PSNR & SSIM across all methods\n",
    "5. **Video processing** — frame-by-frame inference, video reconstruction\n",
    "\n",
    "---\n",
    "**Dependencies:**\n",
    "```\n",
    "pip install torch torchvision numpy scipy matplotlib scikit-image opencv-python Pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage import data as skdata, color, transform\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "from skimage.transform import resize, rescale\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')\n",
    "print(f'PyTorch: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 · Data Pipeline\n",
    "\n",
    "For super resolution:\n",
    "- **HR** (high-res) = original image — ground truth\n",
    "- **LR** (low-res) = HR downsampled by scale factor, then upsampled back to HR size via bicubic\n",
    "- **SRCNN input** = bicubic-upsampled LR image (same size as HR)\n",
    "- **SRCNN output** = refined HR prediction\n",
    "\n",
    "```\n",
    "HR → downsample → LR → bicubic upsample → SRCNN → SR (≈ HR)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Parameters ────────────────────────────────────────────────────────────────\n",
    "SCALE_FACTOR  = 3       # ← tune: 2, 3, or 4 (upscaling factor)\n",
    "PATCH_SIZE    = 33      # ← tune: SRCNN input patch size (must be odd)\n",
    "STRIDE        = 14      # ← tune: patch extraction stride\n",
    "BATCH_SIZE    = 64\n",
    "NUM_EPOCHS    = 30      # ← tune: increase for better results\n",
    "LR            = 1e-4\n",
    "\n",
    "# ── Load clean HR frames ──────────────────────────────────────────────────────\n",
    "def load_hr_frames(size=256):\n",
    "    loaders = [\n",
    "        skdata.astronaut, skdata.camera, skdata.coins, skdata.horse,\n",
    "        skdata.hubble_deep_field, skdata.moon, skdata.page, skdata.text,\n",
    "        skdata.chelsea, skdata.coffee, skdata.immunohistochemistry, skdata.rocket\n",
    "    ]\n",
    "    frames = []\n",
    "    for loader in loaders:\n",
    "        img = loader()\n",
    "        if img.ndim == 3:\n",
    "            img = color.rgb2gray(img)\n",
    "        img = resize(img, (size, size), anti_aliasing=True).astype(np.float32)\n",
    "        frames.append(img)\n",
    "    return frames\n",
    "\n",
    "def make_lr_bicubic(hr, scale):\n",
    "    \"\"\"Downsample HR then upsample back to HR size via bicubic.\"\"\"\n",
    "    lr = rescale(hr, 1.0 / scale, anti_aliasing=True, order=3)\n",
    "    lr_up = resize(lr, hr.shape, order=3, anti_aliasing=False).astype(np.float32)\n",
    "    return lr_up\n",
    "\n",
    "hr_frames = load_hr_frames(size=252)   # 252 divisible by 3 and 4\n",
    "lr_frames = [make_lr_bicubic(f, SCALE_FACTOR) for f in hr_frames]\n",
    "\n",
    "print(f'Loaded {len(hr_frames)} frame pairs — HR shape: {hr_frames[0].shape}')\n",
    "\n",
    "# Preview\n",
    "fig, axes = plt.subplots(2, 6, figsize=(18, 6))\n",
    "for i in range(6):\n",
    "    axes[0, i].imshow(hr_frames[i], cmap='gray'); axes[0, i].set_title(f'HR frame {i+1}'); axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(lr_frames[i], cmap='gray'); axes[1, i].set_title(f'LR×{SCALE_FACTOR} frame {i+1}'); axes[1, i].axis('off')\n",
    "fig.suptitle('HR vs. Bicubic-upsampled LR Input', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Extracts overlapping patches from (LR_bicubic, HR) pairs.\n",
    "    SRCNN is trained on patches to keep memory manageable.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr_frames, hr_frames, patch_size=33, stride=14):\n",
    "        self.lr_patches = []\n",
    "        self.hr_patches = []\n",
    "\n",
    "        for lr, hr in zip(lr_frames, hr_frames):\n",
    "            H, W = hr.shape\n",
    "            for r in range(0, H - patch_size + 1, stride):\n",
    "                for c in range(0, W - patch_size + 1, stride):\n",
    "                    self.lr_patches.append(lr[r:r+patch_size, c:c+patch_size])\n",
    "                    self.hr_patches.append(hr[r:r+patch_size, c:c+patch_size])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr = torch.from_numpy(self.lr_patches[idx]).unsqueeze(0)  # (1, H, W)\n",
    "        hr = torch.from_numpy(self.hr_patches[idx]).unsqueeze(0)\n",
    "        return lr, hr\n",
    "\n",
    "# ── Train / val / test split ──────────────────────────────────────────────────\n",
    "train_lr, train_hr = lr_frames[:8],   hr_frames[:8]\n",
    "val_lr,   val_hr   = lr_frames[8:10], hr_frames[8:10]\n",
    "test_lr,  test_hr  = lr_frames[10:],  hr_frames[10:]\n",
    "\n",
    "train_ds = SRDataset(train_lr, train_hr, PATCH_SIZE, STRIDE)\n",
    "val_ds   = SRDataset(val_lr,   val_hr,   PATCH_SIZE, STRIDE)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f'Train patches: {len(train_ds):,} | Val patches: {len(val_ds):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 · Classical Upsampling Baselines\n",
    "\n",
    "| Method | Order | Characteristic |\n",
    "|---|---|---|\n",
    "| Nearest neighbour | 0 | Fastest; blocky artefacts |\n",
    "| Bilinear | 1 | Smooth but blurry |\n",
    "| Bicubic | 3 | Standard baseline for SR |\n",
    "| Lanczos | 5 | Sharpest classical method |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classical_upsample(hr, scale):\n",
    "    \"\"\"Downsample HR, then upsample with each classical method.\"\"\"\n",
    "    lr_small = rescale(hr, 1.0 / scale, anti_aliasing=True, order=3)\n",
    "    target   = hr.shape\n",
    "    return {\n",
    "        'Nearest':  resize(lr_small, target, order=0).astype(np.float32),\n",
    "        'Bilinear': resize(lr_small, target, order=1).astype(np.float32),\n",
    "        'Bicubic':  resize(lr_small, target, order=3).astype(np.float32),\n",
    "        'Lanczos':  resize(lr_small, target, order=5).astype(np.float32),\n",
    "    }\n",
    "\n",
    "def compute_metrics(ref, candidate):\n",
    "    p = psnr(ref, candidate, data_range=1.0)\n",
    "    s = ssim(ref, candidate, data_range=1.0)\n",
    "    return p, s\n",
    "\n",
    "# Evaluate on first test frame\n",
    "hr_test  = test_hr[0]\n",
    "upscaled = classical_upsample(hr_test, SCALE_FACTOR)\n",
    "\n",
    "print(f'=== Classical Baselines (scale ×{SCALE_FACTOR}) ===')\n",
    "print(f'{\"Method\":<14} {\"PSNR (dB)\":>10} {\"SSIM\":>8}')\n",
    "print('-' * 36)\n",
    "for name, img in upscaled.items():\n",
    "    p, s = compute_metrics(hr_test, img)\n",
    "    print(f'{name:<14} {p:>10.2f} {s:>8.4f}')\n",
    "\n",
    "# Visualize\n",
    "imgs   = [hr_test, *upscaled.values()]\n",
    "titles = ['HR (ground truth)', *upscaled.keys()]\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for ax, img, title in zip(axes, imgs, titles):\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "fig.suptitle(f'Classical Upsampling Baselines (×{SCALE_FACTOR})', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 · SRCNN — Super Resolution CNN\n",
    "\n",
    "**SRCNN** (Dong et al., 2014) is the pioneering CNN for image super resolution.\n",
    "It maps a bicubic-upsampled LR image directly to HR using three conv layers:\n",
    "\n",
    "| Layer | Kernel | Filters | Role |\n",
    "|---|---|---|---|\n",
    "| Conv1 | 9×9 | 64 | Patch extraction & representation |\n",
    "| Conv2 | 1×1 | 32 | Non-linear mapping |\n",
    "| Conv3 | 5×5 | 1  | Reconstruction |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    \"\"\"SRCNN: Learning a Deep Convolutional Network for Image Super-Resolution.\n",
    "    Dong et al., ECCV 2014 / IEEE TPAMI 2015.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=9, padding=4)   # patch extraction\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=0)  # non-linear mapping\n",
    "        self.conv3 = nn.Conv2d(32, 1,  kernel_size=5, padding=2)  # reconstruction\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for layer in [self.conv1, self.conv2]:\n",
    "            nn.init.normal_(layer.weight, mean=0, std=0.001)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        nn.init.normal_(self.conv3.weight, mean=0, std=0.001)\n",
    "        nn.init.zeros_(self.conv3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "model = SRCNN().to(DEVICE)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'SRCNN parameters: {total_params:,}')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 · Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "# SRCNN paper uses different LRs per layer\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.conv1.parameters(), 'lr': LR},\n",
    "    {'params': model.conv2.parameters(), 'lr': LR},\n",
    "    {'params': model.conv3.parameters(), 'lr': LR * 0.1},  # smaller LR for last layer\n",
    "])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ── Train ──────────────────────────────────────────────────────────────────\n",
    "    model.train()\n",
    "    t_loss = 0.0\n",
    "    for lr_patch, hr_patch in train_loader:\n",
    "        lr_patch, hr_patch = lr_patch.to(DEVICE), hr_patch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        sr_patch = model(lr_patch)\n",
    "        loss = criterion(sr_patch, hr_patch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_loss += loss.item()\n",
    "\n",
    "    # ── Validate ───────────────────────────────────────────────────────────────\n",
    "    model.eval()\n",
    "    v_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for lr_patch, hr_patch in val_loader:\n",
    "            lr_patch, hr_patch = lr_patch.to(DEVICE), hr_patch.to(DEVICE)\n",
    "            sr_patch = model(lr_patch)\n",
    "            v_loss += criterion(sr_patch, hr_patch).item()\n",
    "\n",
    "    t_loss /= len(train_loader)\n",
    "    v_loss /= len(val_loader)\n",
    "    train_losses.append(t_loss)\n",
    "    val_losses.append(v_loss)\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save best model\n",
    "    if v_loss < best_val_loss:\n",
    "        best_val_loss = v_loss\n",
    "        torch.save(model.state_dict(), 'srcnn_best.pth')\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1:>3}/{NUM_EPOCHS}]  Train: {t_loss:.6f}  Val: {v_loss:.6f}  LR: {scheduler.get_last_lr()[0]:.2e}')\n",
    "\n",
    "# ── Load best weights & plot ───────────────────────────────────────────────────\n",
    "model.load_state_dict(torch.load('srcnn_best.pth', map_location=DEVICE))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses,   label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('SRCNN Training Curves')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f'Best val loss: {best_val_loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 · Evaluation — Classical vs. SRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def srcnn_upscale(model, lr_bicubic):\n",
    "    \"\"\"Run SRCNN inference on a full bicubic-upsampled frame.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        t = torch.from_numpy(lr_bicubic).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "        out = model(t).squeeze().cpu().numpy()\n",
    "    return np.clip(out, 0, 1).astype(np.float32)\n",
    "\n",
    "print(f'=== Super Resolution Benchmark (×{SCALE_FACTOR}) ===')\n",
    "print(f'{\"Method\":<14} {\"PSNR (dB)\":>10} {\"SSIM\":>8}')\n",
    "print('-' * 36)\n",
    "\n",
    "all_results = {}\n",
    "for hr, lr_bic in zip(test_hr, test_lr):\n",
    "    methods = {**classical_upsample(hr, SCALE_FACTOR), 'SRCNN': srcnn_upscale(model, lr_bic)}\n",
    "    for name, img in methods.items():\n",
    "        p, s = compute_metrics(hr, img)\n",
    "        all_results.setdefault(name, []).append((p, s))\n",
    "\n",
    "for name, scores in all_results.items():\n",
    "    avg_p = np.mean([s[0] for s in scores])\n",
    "    avg_s = np.mean([s[1] for s in scores])\n",
    "    marker = ' ◀ best' if name == 'SRCNN' else ''\n",
    "    print(f'{name:<14} {avg_p:>10.2f} {avg_s:>8.4f}{marker}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visual comparison ─────────────────────────────────────────────────────────\n",
    "hr_test  = test_hr[0]\n",
    "lr_bic   = test_lr[0]\n",
    "upscaled = classical_upsample(hr_test, SCALE_FACTOR)\n",
    "sr_out   = srcnn_upscale(model, lr_bic)\n",
    "\n",
    "imgs   = [hr_test, upscaled['Nearest'], upscaled['Bicubic'], upscaled['Lanczos'], sr_out]\n",
    "titles = ['HR (ground truth)', 'Nearest', 'Bicubic', 'Lanczos', 'SRCNN']\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for ax, img, title in zip(axes, imgs, titles):\n",
    "    p, s = compute_metrics(hr_test, img)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'{title}\\nPSNR={p:.1f} dB', fontsize=9)\n",
    "    ax.axis('off')\n",
    "fig.suptitle(f'Classical vs. SRCNN Super Resolution (×{SCALE_FACTOR})', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ── Zoomed crop comparison ────────────────────────────────────────────────────\n",
    "r, c, s = 80, 80, 80   # ← tune: crop region\n",
    "crops  = [img[r:r+s, c:c+s] for img in imgs]\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for ax, crop, title in zip(axes, crops, titles):\n",
    "    ax.imshow(crop, cmap='gray', interpolation='nearest')\n",
    "    ax.set_title(f'{title} (crop)')\n",
    "    ax.axis('off')\n",
    "fig.suptitle('Zoomed Crop — Fine Detail Comparison', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 · Video Super Resolution Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Generate a synthetic LR video ─────────────────────────────────────────────\n",
    "SYNTHETIC_VIDEO = 'synthetic_lr.avi'\n",
    "H, W = 84, 84   # LR frame size (will be upscaled to 252×252)\n",
    "fps  = 10\n",
    "\n",
    "writer = cv2.VideoWriter(SYNTHETIC_VIDEO, cv2.VideoWriter_fourcc(*'XVID'), fps, (W, H), isColor=False)\n",
    "for hr in (test_hr * 5):\n",
    "    lr_small = rescale(hr, 1.0 / SCALE_FACTOR, anti_aliasing=True, order=3)\n",
    "    uint8 = (lr_small * 255).astype(np.uint8)\n",
    "    writer.write(uint8)\n",
    "writer.release()\n",
    "print(f'LR video saved: {SYNTHETIC_VIDEO}')\n",
    "\n",
    "# ── Option B: use your own video ──────────────────────────────────────────────\n",
    "# SYNTHETIC_VIDEO = 'your_video.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_resolve_video(input_path, output_path, model, scale):\n",
    "    \"\"\"\n",
    "    Read LR video, apply SRCNN super resolution, write HR output video.\n",
    "    \"\"\"\n",
    "    cap   = cv2.VideoCapture(input_path)\n",
    "    fps   = cap.get(cv2.CAP_PROP_FPS)\n",
    "    W_lr  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    H_lr  = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    W_hr, H_hr = W_lr * scale, H_lr * scale\n",
    "\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (W_hr, H_hr), isColor=False)\n",
    "\n",
    "    model.eval()\n",
    "    for _ in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        gray   = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0\n",
    "        lr_bic = resize(gray, (H_hr, W_hr), order=3, anti_aliasing=False).astype(np.float32)\n",
    "        sr     = srcnn_upscale(model, lr_bic)\n",
    "        out.write((sr * 255).astype(np.uint8))\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f'SR video saved: {output_path}  ({W_lr}×{H_lr} → {W_hr}×{H_hr},  {total} frames)')\n",
    "\n",
    "super_resolve_video(SYNTHETIC_VIDEO, 'sr_output.avi', model, SCALE_FACTOR)\n",
    "\n",
    "# ── Visualize sample frames ───────────────────────────────────────────────────\n",
    "cap = cv2.VideoCapture(SYNTHETIC_VIDEO)\n",
    "sample_lr, sample_sr, sample_hr = [], [], []\n",
    "for i, hr in enumerate(test_hr[:5]):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    gray   = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0\n",
    "    lr_bic = resize(gray, hr.shape, order=3).astype(np.float32)\n",
    "    sample_lr.append(lr_bic)\n",
    "    sample_sr.append(srcnn_upscale(model, lr_bic))\n",
    "    sample_hr.append(hr)\n",
    "cap.release()\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(18, 10))\n",
    "for i in range(5):\n",
    "    axes[0, i].imshow(sample_hr[i], cmap='gray'); axes[0, i].set_title(f'HR frame {i+1}');       axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(sample_lr[i], cmap='gray'); axes[1, i].set_title(f'LR bicubic {i+1}');     axes[1, i].axis('off')\n",
    "    axes[2, i].imshow(sample_sr[i], cmap='gray'); axes[2, i].set_title(f'SRCNN SR {i+1}');       axes[2, i].axis('off')\n",
    "fig.suptitle('Video Super Resolution — HR / LR / SRCNN', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7 · Save & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'srcnn_weights.pth')\n",
    "print('Model saved to srcnn_weights.pth')\n",
    "\n",
    "# ── Load (example) ────────────────────────────────────────────────────────────\n",
    "# model_loaded = SRCNN().to(DEVICE)\n",
    "# model_loaded.load_state_dict(torch.load('srcnn_weights.pth', map_location=DEVICE))\n",
    "# model_loaded.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
